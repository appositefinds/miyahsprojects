{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c3701f-8127-48ab-a4cf-1142745145f2",
   "metadata": {},
   "source": [
    "# Assignment 2a: Building a Unigram Language Model in NLP\n",
    "\n",
    "## DTSC-685: Natural Language Processing\n",
    "\n",
    "## Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3785effb-1c39-4eb2-8d62-40032396feec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T19:12:56.277536Z",
     "iopub.status.busy": "2023-11-05T19:12:56.276538Z",
     "iopub.status.idle": "2023-11-05T19:12:56.292547Z",
     "shell.execute_reply": "2023-11-05T19:12:56.290549Z",
     "shell.execute_reply.started": "2023-11-05T19:12:56.277536Z"
    }
   },
   "source": [
    "### Overview\n",
    "\n",
    "Objective: Develop a program that calculates the probability of each word within a provided text corpus using a unigram model.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- 1 - Importing the Data\n",
    "\n",
    "- 2 - Data Cleaning and Tokenization: Preprocess the text by removing special characters and converting everything to lowercase to ensure uniformity. Split the text into individual words (unigrams) to analyze their frequencies.\n",
    "\n",
    "- 3 - Probability Calculation: Determine the probability of each word by dividing its frequency by the total number of words in the text.\n",
    "\n",
    "- 4 - Presentation: Display the top 10 most probable words along with their corresponding probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5280786-c750-4850-a379-ebf1fab0f22f",
   "metadata": {},
   "source": [
    "### 01 - Importing the Data\n",
    "\n",
    "For this assignment, we will be working with Bram Stoker's classic novel \"Dracula,\" published in 1897. Since this book is in the public domain, it is available for use at no cost through the [Gutenberg Project](https://www.gutenberg.org/).\n",
    "\n",
    "The downloaded text file of \"Dracula\" has been provided for you in the text file `pg345.txt`.\n",
    "\n",
    "You will need to create a list named `dracula`, where each element is an individual line from the book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42bbb40-409d-42cf-b7ec-a45fd055543d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1 - Importing the Data\n",
    "with open('pg345.txt', 'r', encoding='utf-8') as file:\n",
    "    dracula = file.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfb879-112a-42e2-bea5-6a34267c9455",
   "metadata": {},
   "source": [
    "### 02 - Data Cleaning\n",
    "\n",
    "In this part of the assignment, we will refine the list `dracula` by converting all the text to lowercase, removing punctuation, and eliminating 'stop words'. To accomplish this, you will create a function named `cleanText` that takes a list of text as input and returns a list of cleaned tokens. Each token in this list should be in lowercase, stripped of punctuation, and free of ['stop words'](https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47).\n",
    "\n",
    "\n",
    "- For the punctuation, import the string library (import string) and use as list (`list(string.punctuation)`).\n",
    "\n",
    "- For the 'stop words', import the library stopword (from nltk.corpus import stopwords), load the 'stop words' (nltk.download('stopwords')) , and use the list stopwords.words('english').\n",
    "\n",
    "- Remove the characters in this list: `[\"“\",\"’\",\"--\",\"”\"]`\n",
    "\n",
    "- Instead of using the split() function to divide the lines into words, it's better to utilize the `word_tokenize` function from `nltk.tokenize` for a more robust tokenization that can handle complex word structures.\n",
    "\n",
    "\n",
    "Output:\n",
    "    \n",
    "    dracula[400:410]\n",
    "    \n",
    "    [['seemed', 'darkness', 'closing', 'upon', 'us', 'great', 'masses'],\n",
    "     ['greyness', 'bestrewed', 'trees', 'produced'],\n",
    "     ['peculiarly', 'weird', 'solemn', 'effect', 'carried', 'thoughts'],\n",
    "     ['grim', 'fancies', 'engendered', 'earlier', 'evening', 'falling', 'sunset'],\n",
    "     ['threw', 'strange', 'relief', 'ghost-like', 'clouds', 'amongst'],\n",
    "     ['carpathians', 'seem', 'wind', 'ceaselessly', 'valleys', 'sometimes'],\n",
    "     ['hills', 'steep', 'despite', 'driver', 'haste', 'horses', 'could'],\n",
    "     ['go', 'slowly', 'wished', 'get', 'walk', 'home'],\n",
    "     ['driver', 'would', 'hear', 'said', 'must'],\n",
    "     ['walk', 'dogs', 'fierce', 'added']]\n",
    "\n",
    "Ps.:\n",
    "    Don't forget to:\n",
    "        \n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d27fcb1c-3102-4e65-ad5e-c6017398c711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MiyahSegura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MiyahSegura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['seemed', 'darkness', 'closing', 'upon', 'us', 'great', 'masses'],\n",
       " ['greyness', 'bestrewed', 'trees', 'produced'],\n",
       " ['peculiarly', 'weird', 'solemn', 'effect', 'carried', 'thoughts'],\n",
       " ['grim', 'fancies', 'engendered', 'earlier', 'evening', 'falling', 'sunset'],\n",
       " ['threw', 'strange', 'relief', 'ghost-like', 'clouds', 'amongst'],\n",
       " ['carpathians', 'seem', 'wind', 'ceaselessly', 'valleys', 'sometimes'],\n",
       " ['hills', 'steep', 'despite', 'driver', 'haste', 'horses', 'could'],\n",
       " ['go', 'slowly', 'wished', 'get', 'walk', 'home'],\n",
       " ['driver', 'would', 'hear', 'said', 'must'],\n",
       " ['walk', 'dogs', 'fierce', 'added']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary nltk data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the function for cleaning text\n",
    "def cleanText(text_lines):\n",
    "    # List of punctuation to remove\n",
    "    punctuation_list = list(string.punctuation) + [\"“\",\"’\",\"--\",\"”\"]\n",
    "    \n",
    "    # Set of English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for line in text_lines:\n",
    "        # Convert to lowercase\n",
    "        line = line.lower()\n",
    "        # Tokenize the line\n",
    "        tokens = word_tokenize(line)\n",
    "        # Remove punctuation and stopwords\n",
    "        cleaned_line = [word for word in tokens if word not in punctuation_list and word not in stop_words]\n",
    "        cleaned_tokens.append(cleaned_line)\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "# Apply the cleaning function to the Dracula text\n",
    "cleaned_dracula = cleanText(dracula)\n",
    "\n",
    "# Output a small sample\n",
    "cleaned_dracula[400:410]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d4e12-058e-4c32-b8ca-3262f24a6800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T20:25:01.529008Z",
     "iopub.status.busy": "2023-11-05T20:25:01.528008Z",
     "iopub.status.idle": "2023-11-05T20:25:01.537019Z",
     "shell.execute_reply": "2023-11-05T20:25:01.536014Z",
     "shell.execute_reply.started": "2023-11-05T20:25:01.529008Z"
    }
   },
   "source": [
    "### 3 - Probability Calculation: \n",
    "\n",
    "In this section, we will calculate the probability of occurrence for each word within our text. This is achieved by dividing the frequency of each word by the total number of words present in the text.\n",
    "\n",
    "To accomplish this, we will utilize a selection of fundamental functions provided by the NLTK library:\n",
    "\n",
    "#### **- padded_everygram_pipeline:**\n",
    "\n",
    "    from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "    \n",
    "    This function is designed to prepare a sequence of textual data for training n-gram language models. It executes two primary operations: padding and the generation of everygrams. Padding entails appending special tokens at the beginning and end of each sentence to delineate sentence boundaries, typically using <s> for the start and </s> for the end. Everygrams are contiguous sequences of tokens, or n-grams, and this function produces all possible n-grams up to a specified size.\n",
    "\n",
    "    **Parameters:**\n",
    "        `n`: The order of the model, which specifies the maximum size of n-grams to be created. For instance, n=2 would create bigrams (and unigrams), n=3 would create trigrams (and bigrams and unigrams), and so on.\n",
    "\n",
    "        `text`: An iterable of lists of strings (sentences), where each string is a token. Essentially, this should be the tokenized text data split into sentences.\n",
    "\n",
    "    **Returns:**\n",
    "        The function returns a tuple with two elements:\n",
    "\n",
    "        `train_data`: An iterable of lists of n-grams represented as tuples. For a unigram model (n=1), this would be an iterable of lists of single-word tuples. For higher-order models, these tuples would contain more words. The iterable is ready to be used as training data for an n-gram model.\n",
    "\n",
    "        `padded_vocab`: An iterator over all the unique tokens (including padding tokens) that appear in the padded n-grams. This iterator can be converted to a list or set to get a vocabulary list that includes the special padding tokens.\n",
    "    \n",
    "\n",
    "\n",
    "#### **- MLE:**\n",
    "\n",
    "    from nltk.lm import MLE    \n",
    "    \n",
    "    The MLE class is used to create an n-gram language model where the probability of each n-gram is estimated using the Maximum Likelihood Estimation approach. This approach calculates the probabilities of n-grams purely based on their frequencies in the training data, without applying any smoothing. In other words, the probability of an n-gram is the number of times it occurs in the training data divided by the total number of occurrences of its (n-1)-gram prefix.\n",
    "\n",
    "    - fit Function: The fit function is used to train the language model using the provided training data.\n",
    "\n",
    "        **Parameters**: `train_data`: An iterable of lists of n-grams, where each n-gram is represented as a tuple. `vocab`: An iterable of all the unique tokens in the training data, which can include padding tokens.\n",
    "\n",
    "        **Returns**: This function does not return a value; instead, it updates the MLE model in place with the probabilities calculated from the training data.\n",
    "\n",
    "\n",
    "    - score Function: The score function is used to calculate the probability of a given n-gram within the model.\n",
    "\n",
    "        **Parameters**: `word`: The n-gram for which the probability is being calculated. For unigram models, this would be a single word.\n",
    "        `context`: (Optional) The preceding words (n-1) of the n-gram for which the probability is calculated. This parameter is not used in unigram models.\n",
    "\n",
    "        **Returns**: A float representing the probability of the given n-gram. If the n-gram has not been seen in the training data, the probability returned is 0, indicating that, according to the model, the n-gram is impossible in the language.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fbd661-0391-475b-85ec-9da991a2167c",
   "metadata": {},
   "source": [
    "Please adhere to the sequence outlined below for your coding task:\n",
    "\n",
    "1. Employ the `padded_everygram_pipeline` function, utilizing the correct parameter, to generate both `train_data` and `padded_vocab`.\n",
    "   \n",
    "2. Transform `padded_vocab` into a list format.\n",
    "\n",
    "3. Instantiate a model of the MLE type, called `unigram_model`, ensuring to apply the relevant parameter.\n",
    "\n",
    "4. Proceed to fit the model using `train_data` in conjunction with `padded_vocab`.\n",
    "\n",
    "5. Construct a dictionary called `unigram_probs` employing the `model.score` method, structured as follows:\n",
    "   \\{ \"word\": probability_of_unigram \\}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192b6fe-4745-4462-b95e-9a8ef873f121",
   "metadata": {},
   "source": [
    "Output:\n",
    "    \n",
    "    unigram_probs (first 10 elements):\n",
    "     \n",
    "    ﻿the 1.3585110718652357e-05\n",
    "    project 0.0012090748539600599\n",
    "    gutenberg 0.0004211384322782231\n",
    "    ebook 0.00017660643934248063\n",
    "    dracula 0.0005298193180274419\n",
    "    use 0.0007335959788072273\n",
    "    anyone 8.151066431191414e-05\n",
    "    anywhere 0.00024453199293574245\n",
    "    united 0.00024453199293574245\n",
    "    states 0.00029887243581035183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37f74aa-6e68-48d2-86b8-acfbde93e41c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿the: 1.3584741618214421e-05\n",
      "project: 0.0012090420040210836\n",
      "gutenberg: 0.00042112699016464707\n",
      "ebook: 0.00017660164103678748\n",
      "dracula: 0.0005298049231103625\n",
      "use: 0.0007335760473835787\n",
      "anyone: 8.150844970928653e-05\n",
      "anywhere: 0.0002445253491278596\n",
      "united: 0.0002445253491278596\n",
      "states: 0.00029886431560071725\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "# Prepare the data for a unigram model\n",
    "n = 1\n",
    "train_data, padded_vocab = padded_everygram_pipeline(n, cleaned_dracula)\n",
    "\n",
    "# Convert the padded vocabulary to a list\n",
    "vocab_list = list(padded_vocab)\n",
    "\n",
    "# Create and train an MLE unigram model\n",
    "unigram_model = MLE(n)\n",
    "unigram_model.fit(train_data, vocab_list)\n",
    "\n",
    "# Calculate the unigram probabilities\n",
    "unigram_probs = {word: unigram_model.score(word) for word in vocab_list}\n",
    "\n",
    "# Output the first 10 unigram probabilities\n",
    "for i, (word, prob) in enumerate(unigram_probs.items()):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(f\"{word}: {prob}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b1204-a9ef-42e9-984c-3c78f6cb51a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T21:07:54.089930Z",
     "iopub.status.busy": "2023-11-05T21:07:54.088936Z",
     "iopub.status.idle": "2023-11-05T21:07:54.103450Z",
     "shell.execute_reply": "2023-11-05T21:07:54.102497Z",
     "shell.execute_reply.started": "2023-11-05T21:07:54.089930Z"
    }
   },
   "source": [
    "### 4 - Presentation: \n",
    "\n",
    "Display the top 10 most probable words along with their corresponding probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2639c59c-1e50-4749-a685-d5540f3c7a16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most probable words:\n",
      "said: 0.0077433027223822205\n",
      "one: 0.006846709775580068\n",
      "could: 0.0066836928761614955\n",
      "us: 0.006262565885996848\n",
      "must: 0.00611313372819649\n",
      "would: 0.00586860837906863\n",
      "shall: 0.005814269412595772\n",
      "may: 0.005637667771558985\n",
      "see: 0.00540672716404934\n",
      "know: 0.005298049231103624\n"
     ]
    }
   ],
   "source": [
    "# Sort the unigram probabilities in descending order\n",
    "sorted_unigrams = sorted(unigram_probs.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Display the top 10 most probable words\n",
    "print(\"Top 10 most probable words:\")\n",
    "for word, prob in sorted_unigrams[:10]:\n",
    "    print(f\"{word}: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b416706-68e9-4e61-9233-4564bf578d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b91ec316-0cf0-446c-82f5-813ef5e8bbfc",
   "metadata": {},
   "source": [
    "### 5 - Export Models for codegrade evaluation\n",
    "\n",
    "Using the \"pickle\" library:\n",
    "\n",
    "- Export the model `unigram_model` as \"unigram_model.pkl\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f354947-dcfe-4522-b97a-409c12e2a0a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Save the models in disk\n",
    "with open('unigram_model.pkl', 'wb') as file:\n",
    "    pickle.dump(unigram_model , file)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e55882-8acc-4ddb-9984-4acb3ac82a51",
   "metadata": {},
   "source": [
    "This material is for enrolled students' academic use only and protected under U.S. Copyright Laws. This content must not be shared outside the confines of this course, in line with Eastern University's academic integrity policies. Unauthorized reproduction, distribution, or transmission of this material, including but not limited to posting on third-party platforms like GitHub, is strictly prohibited and may lead to disciplinary action. You may not alter or remove any copyright or other notice from copies of any content taken from BrightSpace or Eastern University’s website.\n",
    "\n",
    "© Copyright Notice 2024, Eastern University - All Rights Reserved"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
